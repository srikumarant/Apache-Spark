
- Read data from HDFS
- Apply map transformation to convert each record into tuple
- Use toDF function to convert RDD into Data Frame
- Register Data Frame as temp table
- Run queries against temp table using sqlContext.sql


/--Create Data Frame and Register as temp table--/

import org.apache.spark.sql.SQLContext

//Read data from ## HDFS ##
val ordersRDD = sc.textFile("/public/retail_db/orders")

//Apply map transformation to convert each record into tuple
//Use toDF function to convert RDD into Data Frame
val ordersDF = ordersRDD.map(order => {
  (order.split(",")(0).toInt, order.split(",")(1), order.split(",")(2).toInt, order.split(",")(3))
}).toDF("order_id", "order_date", "order_customer_id", "order_status")

//Register Data Frame as temp table
ordersDF.registerTempTable("orders")

//Run queries against temp table using sqlContext.sql
val sqlcontext = new org.apache.spark.sql.SQLContext(sc)
sqlContext.sql("use sri_retail_db_orc")
sqlContext.sql("select order_status, count(1) count_by_status from orders group by order_status").show()

--

//Read data from ## Local File System ##
val productsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
val productsRDD = sc.parallelize(productsRaw)

//Apply map transformation to convert each record into tuple
//Use toDF function to convert RDD into Data Frame
val productsDF = productsRDD.map(product => {
  (product.split(",")(0).toInt, product.split(",")(2))
}).toDF("product_id", "product_name")

//Register Data Frame as temp table
productsDF.registerTempTable("products")

//Run queries against temp table using sqlContext.sql
val sqlcontext = new org.apache.spark.sql.SQLContext(sc)
sqlContext.sql("select * from products").show()

--

//Read data from ## mySQL ##
sqlContext.setConf("spark.sql.shuffle.partitions", "10")
val ordersDF = sqlContext.load("jdbc", Map("url" -> "jdbc:mysql://localhost:3306/retail_db?user=root&password=hadoop", "dbtable" -> "orders"))

//val ordersDF = sqlContext.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/retail_db").option("driver", "com.mysql.jdbc.Driver").option("dbtable", "orders").option("user", "root").option("password", "hadoop").option("fetchSize","1000000").load()

//Register Data Frame as temp table
ordersDF.registerTempTable("orders")


/--Process data--/

ordersDF.registerTempTable("orders")
productsDF.registerTempTable("products")
sqlContext.sql("show tables").show()

sqlContext.sql("SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) daily_revenue_per_product " +
	       "FROM orders o JOIN order_items oi " +
	       "ON o.order_id = oi.order_item_order_id " +
	       "JOIN products p ON p.product_id = oi.order_item_product_id " +
	       "WHERE o.order_status IN ('COMPLETE', 'CLOSED') " +
	       "GROUP BY o.order_date, p.product_name " +
	       "ORDER BY o.order_date, daily_revenue_per_product desc").show

sqlContext.sql("use sri_retail_db_orc")
sqlContext.sql("show tables").show()

sqlContext.setConf("spark.sql.shuffle.partitions", "2")


/--Saving Data Frame--/

val daily_revenue_per_product = sqlContext.sql("SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) daily_revenue_per_product " +
	                                       "FROM orders o JOIN order_items oi " +
					       "ON o.order_id = oi.order_item_order_id " +
					       "JOIN products p ON p.product_id = oi.order_item_product_id " +
					       "WHERE o.order_status IN ('COMPLETE', 'CLOSED') " +
					       "GROUP BY o.order_date, p.product_name " +
					       "ORDER BY o.order_date, daily_revenue_per_product desc")

sqlContext.sql("CREATE DATABASE sri_daily_revenue")
sqlContext.sql("CREATE TABLE sri_daily_revenue.daily_revenue " +
	       "(order_date string, product_name string, daily_revenue_per_product float) " +
	       "STORED AS orc")

daily_revenue_per_product.insertInto("sri_daily_revenue.daily_revenue")

sqlContext.sql("select * from sri_daily_revenue.daily_revenue").show


/--Data Frame Operations--/

daily_revenue_per_product.save("/user/sri/daily_revenue_save", "json")
daily_revenue_per_product.write.json("/user/sri/daily_revenue_write")
daily_revenue_per_product.select("order_date", "daily_revenue_per_product")
daily_revenue_per_product.filter(daily_revenue_per_product("order_date") === "2013-07-25 00:00:00.0").count
